{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import math\n",
    "import re\n",
    "import operator\n",
    "import os\n",
    "from os import path\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_documents(dirname):\n",
    "        # Get all files from given directory\n",
    "        files = [dirname+'/'+f for f in os.listdir(dirname)]\n",
    "        documents = []\n",
    "        for filename in files:\n",
    "            # Open file\n",
    "            txt = open(filename)\n",
    "            # Read file as text and add it to String array\n",
    "            documents.append(txt.read())\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "        return [t.lower() for t in re.findall(r\"\\w+(?:[-']\\w+)*\", document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def stem(tokens):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        return [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_doc_frequencies(docs):\n",
    "        res = defaultdict(lambda: 0);\n",
    "        for i in range(len(docs)):\n",
    "            doc = list(set(docs[i]))\n",
    "            for j in range(len(doc)):\n",
    "                res[doc[j]] += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tfidf_index(docs, doc_freqs):\n",
    "        index = defaultdict(list)\n",
    "        total_docs = len(docs)\n",
    "        for i in range(len(docs)):\n",
    "            term_count = dict(Counter(docs[i]))\n",
    "            for term in term_count:\n",
    "                if term_count[term]/len(docs[i])>0:\n",
    "                    index[term].append([i,(1+math.log10(term_count[term]/len(docs[i])))*math.log10(total_docs/doc_freqs[term])])\n",
    "                else:\n",
    "                    index[term].append([i,math.log10(total_docs/doc_freqs[term])])\n",
    "                    \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_doc_lengths(index):\n",
    "        lengths = defaultdict(lambda: 0)\n",
    "        for i in index:\n",
    "            for term_count in index[i]:\n",
    "                lengths[term_count[0]] += math.pow(term_count[1],2)\n",
    "        for key, value in lengths.items():\n",
    "            lengths[key] = math.sqrt(value)\n",
    "        return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_to_vector(query_terms):\n",
    "        return dict(Counter(query_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_by_cosine(query_vector, index, doc_lengths):\n",
    "        scores = defaultdict(lambda: 0)\n",
    "        for query_term, query_weight in query_vector.items():\n",
    "            for doc_id, doc_weight in index[query_term]:\n",
    "                scores[doc_id] += query_weight * doc_weight \n",
    "        for doc_id in scores:\n",
    "            scores[doc_id] /= doc_lengths[doc_id]\n",
    "        return sorted(scores.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_job_info(doc_ids):\n",
    "    res = ''\n",
    "    for doc_id, score in doc_ids[:5]:\n",
    "        fileName = \"jobs/job\" + str(doc_id) + \".txt\"\n",
    "        with open(fileName) as resultFile:\n",
    "            job_info = [next(resultFile) for x in xrange(3)]\n",
    "        print(job_info)\n",
    "\n",
    "\n",
    "def print_user_info(doc_ids):\n",
    "    res = ''\n",
    "    for doc_id, score in doc_ids[:5]:\n",
    "        fileName = \"users/user\" + str(doc_id) + \".txt\"\n",
    "        with open(fileName) as resultFile:\n",
    "            user_info = [next(resultFile) for x in xrange(3)]\n",
    "        print(user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-65d5113846ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jobs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstemmed_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_doc_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tfidf_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_freqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_doc_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1d757ce745f5>\u001b[0m in \u001b[0;36mread_documents\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;31m# Get all files from given directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jobs'"
     ]
    }
   ],
   "source": [
    "documents = read_documents('jobs')\n",
    "stemmed_docs = [tokenize(d) for d in documents]\n",
    "doc_freqs = count_doc_frequencies(stemmed_docs)\n",
    "index = create_tfidf_index(stemmed_docs, doc_freqs)\n",
    "doc_lengths = compute_doc_lengths(index)\n",
    "user_documents = read_documents('users')\n",
    "print(user_documents[0])\n",
    "user_stemmed_docs = tokenize(user_documents[0])\n",
    "user_query_vector = query_to_vector(user_stemmed_docs)\n",
    "search_job_results = search_by_cosine(user_query_vector,index,doc_lengths)\n",
    "print_job_info(search_job_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
